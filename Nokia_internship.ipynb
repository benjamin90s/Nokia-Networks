{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SOTj3Q4kF4Ty",
    "outputId": "d905f359-bfe7-4459-daa6-5310570c93a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.32.1)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.3.1+cu121)\n",
      "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.23.5)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10.0->accelerate)\n",
      "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10.0->accelerate)\n",
      "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10.0->accelerate)\n",
      "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10.0->accelerate)\n",
      "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10.0->accelerate)\n",
      "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10.0->accelerate)\n",
      "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.10.0->accelerate)\n",
      "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
      "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.3.1)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m69.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.7.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
      "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Zmd8eW3MMKR",
    "outputId": "a05952be-8081-4af4-cdf4-0316875c96a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.23.5)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2024.5.15)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.4)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.32.1)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.3.1+cu121)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (1.13.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.3.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->transformers[torch]) (12.5.82)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2024.7.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->transformers[torch]) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->transformers[torch]) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers[torch]\n",
    "#!pip install transformers[torch] - U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 529,
     "referenced_widgets": [
      "211b9adca498428d8a7c734c782b3324",
      "d9b9ca47da634259827e4afd19bf19c8",
      "35ea1605af4443859b60c7a76cc9fd15",
      "8556bc175a4f4c88864b9665d347b1f3",
      "1cc688bd692e4afdb0ae97014de3c3da",
      "4b4b95675db741f7b9b6d4f8b5be1a0c",
      "aa73022ced4f4e74bb97a49815691d14",
      "197e4a236bb64e20bd535a83459a5247",
      "268035c5caa74958b362e668a9a219aa",
      "1c753b9498b84df3bb3a8f57ed1be804",
      "41834f353eb64778930c95ce9d26f152",
      "af0d65d6a17f4e46b0a1e11ebefb0ea8",
      "a0a828cba86e428480b32fdf8d58ce43",
      "241c31cb663b4dca8cff65ea49604314",
      "9ae00062821241f9ae20be00e8521455",
      "bf1cfd87141847ef9d55a898baaf6390",
      "7c91a04a8cc541559422a8f31a3e4b60",
      "034a6586ab524e318f49c8b6c256646c",
      "6aa7c801180b4ceabfd4a7483e04358c",
      "06f3b46e835444a8ba6cd8075b2e6291",
      "49ff165ec2c644c6bc307d959555fe8d",
      "18c455712daf47138f65cdac464a4632",
      "6fa60dbc8f944f6f83db42fdc412f698",
      "67f630da228e43c597d9114224cd0349",
      "9b0ca9f1b21d4b83acda0e664925fca5",
      "213f4ac58142433abab01529c591f8ef",
      "d9d5957c4bc349aeb819f1b8b3302485",
      "5721ed89abb349669874c18a99d3198f",
      "3450448637344c9e800075b379079ce4",
      "c2163b0eee5c463fb079e8d69f2ad879",
      "209cd5774e3f412289d5f2cb9f69893a",
      "b73a15382fc344d5a1f3ce8711b611a6",
      "e102154e876143ed97288217188d6b70",
      "c37139c735b94530b1902fe93d3dbf60",
      "0fba9511e7cb453da2b660742d61ef05",
      "78a794f15a3e4b239a145736fbdc9e37",
      "981033cd10d848d0929d3c76c5c6f7f2",
      "ce7af707c9fd4326b8e0fc8fceea7797",
      "2707d111c1064cb7a4824580ac511f2b",
      "05aa877af1214b03b6be7fd304b7faf7",
      "081abece04db4d62b8cb50b899eef45e",
      "dc9d20257cdc4d9a8781c8ca4a34c61e",
      "2446436c83e7458d87e4cd1c60eccd5d",
      "6cb9a13caedb43ce93122decc23ab851",
      "7da267c9626342318bc82917c09bb217",
      "548e23331f4948e68d6a76d18f06ca1b",
      "94e74265e9214ea6bb2fe796fadc983c",
      "ac01185f571742c793b7aeaa2d1d8308",
      "f629dc387eeb45e9a211de4e2c678b31",
      "ad599ffc018e43aeb3467a49a32adc5e",
      "ad9f850926934dd6999ae3290f97be96",
      "a2d483152851445f9e15c154e6cce0d6",
      "4703b4319ec348dfaec789532267d41e",
      "6f7c2e50f57741bcb89035ca7e4c85e2",
      "adc34181baf646ea9fd4768e7ab8b64a"
     ]
    },
    "id": "mVF-AZQ6rEbZ",
    "outputId": "3a6868a2-8a37-407f-915f-87d2f6492388"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "211b9adca498428d8a7c734c782b3324",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af0d65d6a17f4e46b0a1e11ebefb0ea8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fa60dbc8f944f6f83db42fdc412f698",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c37139c735b94530b1902fe93d3dbf60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7da267c9626342318bc82917c09bb217",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 02:05, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.683100</td>\n",
       "      <td>0.513730</td>\n",
       "      <td>0.757962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.367000</td>\n",
       "      <td>0.295651</td>\n",
       "      <td>0.891720</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 0.89171974522293\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "import random\n",
    "\n",
    "class TextPairDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Concatenate the group headers with the requirement descriptions\n",
    "        text_pair_1 = self.texts[idx][0] + \" [SEP] \" + self.texts[idx][2]  # R_RequirementDesc + R_ReqGroupHeader\n",
    "        text_pair_2 = self.texts[idx][1] + \" [SEP] \" + self.texts[idx][3]  # L_RequirementDesc + L_ReqGroupHeader\n",
    "        encoded_pair = self.tokenizer(\n",
    "            text_pair_1, text_pair_2,\n",
    "            truncation=True, padding='max_length', max_length=512,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        item = {key: val.squeeze(0) for key, val in encoded_pair.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    return {'accuracy': acc}\n",
    "\n",
    "df = pd.read_csv('/content/drive/MyDrive/SampleDataCSL_TSRCoPandVSA.csv')\n",
    "\n",
    "\n",
    "df_y = df[df['Mapped'] == 'Y']\n",
    "\n",
    "\n",
    "df_n = df[df['Mapped'] == 'N'].sample(n=400, random_state=42)  # random_state is set for reproducibility\n",
    "\n",
    "\n",
    "df_combined = pd.concat([df_y, df_n]).dropna(subset=['RequirementDesc', 'RequirementDesc.1', 'L_ReqGroupHeader', 'R_ReqGroupHeader'])\n",
    "df_combined.reset_index(drop=True, inplace=True)\n",
    "df_combined['is_match'] = df_combined['Mapped'].apply(lambda x: 1 if x == 'Y' else 0)\n",
    "\n",
    "# Prepare text pairs, including group headers\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    list(zip(df_combined['RequirementDesc'], df_combined['RequirementDesc.1'], df_combined['R_ReqGroupHeader'], df_combined['L_ReqGroupHeader'])),\n",
    "    df_combined['is_match'].tolist(),\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train_dataset = TextPairDataset(train_texts, train_labels)\n",
    "test_dataset = TextPairDataset(test_texts, test_labels)\n",
    "\n",
    "# Initialize model and training\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',            # Directory where the results and checkpoints will be saved\n",
    "    num_train_epochs=5,                # Total number of training epochs\n",
    "    per_device_train_batch_size=32,    # Batch size per device\n",
    "    warmup_steps=250,                  # Number of warmup steps\n",
    "    weight_decay=0.01,                 # Weight decay\n",
    "    logging_dir='./logs',              # Directory for storing logs\n",
    "    evaluation_strategy=\"steps\",       # Evaluate every `logging_steps`\n",
    "    save_strategy=\"steps\",             # Save the model every `save_steps`\n",
    "    save_steps=100,                    # Save the model every 100 steps\n",
    "    logging_steps=50,                  # Log metrics every 50 steps\n",
    "    load_best_model_at_end=True        # Load the best model at the end of training\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "final_results = trainer.evaluate(test_dataset)\n",
    "print(\"Test Set Accuracy:\", final_results['eval_accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 261
    },
    "id": "Vmwu26GOvvhm",
    "outputId": "3dcf4a14-7860-4b15-f5d9-23abbeafb2ff"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='315' max='315' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [315/315 03:56, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.387800</td>\n",
       "      <td>0.355136</td>\n",
       "      <td>0.875598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.204100</td>\n",
       "      <td>0.320648</td>\n",
       "      <td>0.899522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.191300</td>\n",
       "      <td>0.308463</td>\n",
       "      <td>0.923445</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='53' max='53' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [53/53 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 0.9234449760765551\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Load new dataset\n",
    "new_data_path = '/content/drive/MyDrive/DataSet_2024_07_20.csv'  # Update this path to your new data file\n",
    "new_df = pd.read_csv(new_data_path)\n",
    "\n",
    "# Filter and sample the data\n",
    "df_y = new_df[new_df['Mapped'] == 'Y']\n",
    "df_n = new_df[new_df['Mapped'] == 'N'].sample(n=1000, random_state=42)  # Sample 1000 'N' Mapped entries\n",
    "\n",
    "# Combine and prepare the data\n",
    "df_combined = pd.concat([df_y, df_n]).reset_index(drop=True)\n",
    "df_combined['is_match'] = df_combined['Mapped'].apply(lambda x: 1 if x == 'Y' else 0)\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    list(zip(df_combined['L_RequirementDesc'], df_combined['R_RequirementDesc'], df_combined['R_ReqGroupHeader'], df_combined['L_ReqGroupHeader'])),\n",
    "    df_combined['is_match'].tolist(),\n",
    "    test_size=0.2,  # 20% test data\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Load the saved model\n",
    "model_path = '/content/results/checkpoint-100'\n",
    "model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "# Dataset class\n",
    "class TextPairDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text_pair_1 = self.texts[idx][0] + \" [SEP] \" + self.texts[idx][2]\n",
    "        text_pair_2 = self.texts[idx][1] + \" [SEP] \" + self.texts[idx][3]\n",
    "        encoded_pair = self.tokenizer(\n",
    "            text_pair_1, text_pair_2,\n",
    "            truncation=True, padding='max_length', max_length=512,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        item = {key: val.squeeze(0) for key, val in encoded_pair.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "train_dataset = TextPairDataset(train_texts, train_labels)\n",
    "test_dataset = TextPairDataset(test_texts, test_labels)\n",
    "\n",
    "# Update training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./new_results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./new_logs',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=10\n",
    ")\n",
    "\n",
    "# Define compute_metrics function to evaluate the model\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return {'accuracy': accuracy_score(labels, predictions)}\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "final_results = trainer.evaluate(test_dataset)\n",
    "print(\"Test Set Accuracy:\", final_results['eval_accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i3bYUDEGlTxp",
    "outputId": "90aeb19b-fbfd-4b91-a757-4f3769de1b02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "model_path = '/content/results/checkpoint-100'\n",
    "model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "df = pd.read_csv('/content/drive/MyDrive/SampleDataCSL_TSRCoPandVSA.csv')\n",
    "df['RequirementDesc'] = df['RequirementDesc'].apply(lambda x: x.lower().strip())\n",
    "df['RequirementDesc.1'] = df['RequirementDesc.1'].apply(lambda x: x.lower().strip())\n",
    "\n",
    "texts = list(zip(df['RequirementDesc'], df['RequirementDesc.1']))\n",
    "labels = df['Mapped'].apply(lambda x: 1 if x != 'N' else 0).tolist()\n",
    "\n",
    "\n",
    "encodings = tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "# Create a TensorDataset and DataLoader\n",
    "input_ids = encodings['input_ids']\n",
    "attention_mask = encodings['attention_mask']\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "dataset = TensorDataset(input_ids, attention_mask, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "all_predictions = []\n",
    "similarity_scores = []\n",
    "\n",
    "# Run inference\n",
    "with torch.no_grad():\n",
    "    for batch in dataloader:\n",
    "        input_ids, attention_mask, _ = batch\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "\n",
    "\n",
    "        match_probabilities = probs[:, 1]  # Assuming class '1' is the match class\n",
    "        similarity_scores.extend(match_probabilities.tolist())\n",
    "\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "\n",
    "\n",
    "df['IsCorrectMapping'] = ['Yes' if pred == 1 else 'No' for pred in all_predictions]\n",
    "\n",
    "df['SimilarityScores'] = similarity_scores\n",
    "\n",
    "\n",
    "best_alternative_match = []\n",
    "for i, (mapping, desc1) in enumerate(zip(df['IsCorrectMapping'], df['RequirementDesc.1'])):\n",
    "    if mapping == 'No':\n",
    "        max_index = df[(df['IsCorrectMapping'] == 'No') & (df.index != i)]['SimilarityScores'].idxmax()\n",
    "        best_alternative_match.append(df.loc[max_index, 'RequirementDesc.1'])\n",
    "    else:\n",
    "        best_alternative_match.append(desc1)\n",
    "\n",
    "df['BestMatch_RequirementDesc'] = best_alternative_match\n",
    "\n",
    "df.to_csv('/content/sample_data/Model_SampleDataCSL_TSRCoPandVSA(SimSco).csv', index=False)\n",
    "print(\"DataFrame saved\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
